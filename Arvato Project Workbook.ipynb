{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Create a Customer Segmentation Report for Arvato Financial Services\n",
    "\n",
    "In this project, you will analyze demographics data for customers of a mail-order sales company in Germany, comparing it against demographics information for the general population. You'll use unsupervised learning techniques to perform customer segmentation, identifying the parts of the population that best describe the core customer base of the company. Then, you'll apply what you've learned on a third dataset with demographics information for targets of a marketing campaign for the company, and use a model to predict which individuals are most likely to convert into becoming customers for the company. The data that you will use has been provided by our partners at Bertelsmann Arvato Analytics, and represents a real-life data science task.\n",
    "\n",
    "If you completed the first term of this program, you will be familiar with the first part of this project, from the unsupervised learning project. The versions of those two datasets used in this project will include many more features and has not been pre-cleaned. You are also free to choose whatever approach you'd like to analyzing the data rather than follow pre-determined steps. In your work on this project, make sure that you carefully document your steps and decisions, since your main deliverable for this project will be a blog post reporting your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries here; add more as necessary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "\n",
    "# magic word for producing visualizations in notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Get to Know the Data\n",
    "\n",
    "There are four data files associated with this project:\n",
    "\n",
    "- `Udacity_AZDIAS_052018.csv`: Demographics data for the general population of Germany; 891 211 persons (rows) x 366 features (columns).\n",
    "- `Udacity_CUSTOMERS_052018.csv`: Demographics data for customers of a mail-order company; 191 652 persons (rows) x 369 features (columns).\n",
    "- `Udacity_MAILOUT_052018_TRAIN.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 982 persons (rows) x 367 (columns).\n",
    "- `Udacity_MAILOUT_052018_TEST.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 833 persons (rows) x 366 (columns).\n",
    "\n",
    "Each row of the demographics files represents a single person, but also includes information outside of individuals, including information about their household, building, and neighborhood. Use the information from the first two files to figure out how customers (\"CUSTOMERS\") are similar to or differ from the general population at large (\"AZDIAS\"), then use your analysis to make predictions on the other two files (\"MAILOUT\"), predicting which recipients are most likely to become a customer for the mail-order company.\n",
    "\n",
    "The \"CUSTOMERS\" file contains three extra columns ('CUSTOMER_GROUP', 'ONLINE_PURCHASE', and 'PRODUCT_GROUP'), which provide broad information about the customers depicted in the file. The original \"MAILOUT\" file included one additional column, \"RESPONSE\", which indicated whether or not each recipient became a customer of the company. For the \"TRAIN\" subset, this column has been retained, but in the \"TEST\" subset it has been removed; it is against that withheld column that your final predictions will be assessed in the Kaggle competition.\n",
    "\n",
    "Otherwise, all of the remaining columns are the same between the three data files. For more information about the columns depicted in the files, you can refer to two Excel spreadsheets provided in the workspace. [One of them](./DIAS Information Levels - Attributes 2017.xlsx) is a top-level list of attributes and descriptions, organized by informational category. [The other](./DIAS Attributes - Values 2017.xlsx) is a detailed mapping of data values for each feature in alphabetical order.\n",
    "\n",
    "In the below cell, we've provided some initial code to load in the first two datasets. Note for all of the `.csv` data files in this project that they're semicolon (`;`) delimited, so an additional argument in the [`read_csv()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) call has been included to read in the data properly. Also, considering the size of the datasets, it may take some time for them to load completely.\n",
    "\n",
    "You'll notice when the data is loaded in that a warning message will immediately pop up. Before you really start digging into the modeling and analysis, you're going to need to perform some cleaning. Take some time to browse the structure of the data and look over the informational spreadsheets to understand the data values. Make some decisions on which features to keep, which features to drop, and if any revisions need to be made on data formats. It'll be a good idea to create a function with pre-processing steps, since you'll need to clean all of the datasets before you work with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data\n",
    "#azdias = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_AZDIAS_052018.csv', sep=';')\n",
    "#customers = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_CUSTOMERS_052018.csv', sep=';')\n",
    "azdias = pd.read_csv('arvato_data/Udacity_AZDIAS_052018.csv', sep=';')\n",
    "#customers = pd.read_csv('arvato_data/Udacity_CUSTOMERS_052018.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks we had some trouble already while importing with some features that have mixed types values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These two featutres have mixed type float and string\n",
    "azdias.iloc[:,18:20].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.CAMEO_DEUG_2015.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.CAMEO_INTL_2015.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows a problem with this table where there are same values that are different because of the type either int or fload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fix it straightaway\n",
    "def fix_cameo (df):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    df['CAMEO_DEUG_2015'] = df['CAMEO_DEUG_2015'].replace({'X': np.nan})\n",
    "    df['CAMEO_DEUG_2015'] = df['CAMEO_DEUG_2015'].astype(float)\n",
    "    df['CAMEO_INTL_2015'] = df['CAMEO_INTL_2015'].replace({'XX':np.nan})\n",
    "    df['CAMEO_INTL_2015'] = df['CAMEO_INTL_2015'].astype(float)\n",
    "    return df\n",
    "\n",
    "azdias = fix_cameo(azdias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the values X and XX in these two tables are to be considered \"unknown\" and converted to null. So in our initial cleaning we will check the values of the documented features against the documentation ( the \"DIAS Attributes - Values 2017.xlsx\" file) and consider the ones that fall outside the reanges priveded as null.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the features description excel file into a Dataframe\n",
    "feat_info = pd.read_excel('DIAS Attributes - Values 2017.xlsx', sheet_name='Tabelle1', index_col=[0, 1, 2]).reset_index()\n",
    "feat_info.drop('level_0', axis=1, inplace=True)\n",
    "feat_info_levels = pd.read_excel('DIAS Information Levels - Attributes 2017.xlsx', index_col=[0, 1]).reset_index()\n",
    "feat_info_levels.drop('level_0', axis=1, inplace=True) \n",
    "\n",
    "feat_info_levels.head()\n",
    "feat_info[feat_info.Attribute=='CAMEO_DEUG_2015']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turns out the feat_info dataset from the Excel spreadsheet has some columns that are not in the population dataset\n",
    "# and vicecersa, Let's explore a bit\n",
    "# These one are in the population and are described in the spreadsheet\n",
    "attributes_we_have_info_about = np.intersect1d(feat_info.Attribute.unique(), np.array(azdias.columns))\n",
    "# These one are in the population and are NOT described in the spreadsheet\n",
    "attributes_we_dont_have_info = np.setdiff1d(np.array(azdias.columns), feat_info.Attribute.unique())\n",
    "# These one are in the spreadsheet and are NOT in the population so we just ignore them for now\n",
    "attributes_we_dont_care = np.setdiff1d(feat_info.Attribute.unique(), np.array(azdias.columns))\n",
    "\n",
    "len(attributes_we_have_info_about), len(attributes_we_dont_have_info), len(attributes_we_dont_care)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the descriptions in the included excel spreadsheet, we will build a azdias_info dataframe where we will also add the type of the feature. Since not all features are included in the spreadsheet, we will have to add those to the dataframe, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can build a dataframe with atributes and type plus description of unknown values \n",
    "unknown_values = ['unknown', 'unknown / no main age detectable', 'no transactions known', 'numeric value (typically coded from 1-10)',\n",
    "                 'numeric value (typically coded from 1-3)', 'no transaction known']\n",
    "azdias_info =  feat_info[(feat_info.Attribute.isin(attributes_we_have_info_about)) & (feat_info.Meaning.isin(unknown_values))]\n",
    "# Add the ones who don't have unknown values\n",
    "\n",
    "# Add columns that are in the excel spreadhseet but don't have a \"unkmown value\"\n",
    "for feat in np.setdiff1d(feat_info.Attribute[attributes_we_have_info_about].index, azdias_info.Attribute):\n",
    "    azdias_info = azdias_info.append(feat_info[feat_info.Attribute == feat].iloc[0,:2])\n",
    "\n",
    "# Finally add columns missing if the excel spreadsheet\n",
    "azdias_info = pd.concat([azdias_info, pd.DataFrame(np.setdiff1d(azdias.columns.values, azdias_info.Attribute), columns=['Attribute'])], sort=False, axis=0)\n",
    "\n",
    "# Adding a Type column initially setting everything to 'Ordinal' \n",
    "azdias_info['Type'] = 'Ordinal'\n",
    "azdias_info.to_csv('azdias_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload dataframe after working on it in Excel\n",
    "azdias_info = pd.read_csv('azdias_info_reload.csv', index_col=[0])\n",
    "\n",
    "azdias_info.head()\n",
    "# The Value column has values that are to be considered nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Our info dataframe \\\"azdias_info\\\" has descriptions for {} features out of {}\".format(azdias_info.shape[0], azdias.shape[1]))\n",
    "azdias_info.Type.value_counts()\n",
    "ordinal_features = azdias_info[azdias_info.Type == 'Ordinal'].Attribute.values\n",
    "print(\"Dataset has {} ordinal features\".format(len(ordinal_features)))\n",
    "categorical_features = azdias_info[azdias_info.Type == 'Categorical'].Attribute.values\n",
    "print(\"Dataset has {} categorical features\".format(len(categorical_features)))\n",
    "numeric_features = azdias_info[azdias_info.Type == 'Numeric'].Attribute.values\n",
    "print(\"Dataset has {} numeric features\".format(len(numeric_features)))\n",
    "binary_features =   azdias_info[azdias_info.Type == 'Binary'].Attribute.values  \n",
    "print(\"Dataset has {} binary features\".format(len(binary_features)))\n",
    "# Mix-Type features combine information over 2 axes and will be engineered separately\n",
    "mix_type_features = azdias_info[azdias_info.Type == 'Mix-Type'].Attribute.values \n",
    "print(\"Dataset has {} Mix-Type features\".format(len(mix_type_features)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we will start handling the features that are documented for now and run some cleaning scripts, then we will explore the undocumented ones and decide what to do with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some utility functions\n",
    "# split a string is a string otherwise return unchanged\n",
    "def split_if_string(x):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    if isinstance(x, str):\n",
    "        return x.split(',')\n",
    "    else:\n",
    "        return x\n",
    "# Change ints into floats in a list and leave anything else unchanged\n",
    "def to_float(x):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        x = float(x)\n",
    "    except:\n",
    "        pass\n",
    "    return x\n",
    "\n",
    "def is_number(s):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will look for \"undocumented\" null values, that are not included in the spreadsheet\n",
    "\n",
    "attributes_to_check = np.setdiff1d(attributes_we_have_info_about, numeric_features)\n",
    "\n",
    "# Better align values first as it is taking as different int and floats\n",
    "\n",
    "def check_all_values(df, feat_info):\n",
    "    for attribute in attributes_to_check:\n",
    "        theoretical_vals = feat_info[feat_info.Attribute == attribute].Value.apply(lambda x: split_if_string(x)).values\n",
    "        theoretical_vals = np.hstack(theoretical_vals)\n",
    "        theoretical_vals = [to_float(x) for x in theoretical_vals]\n",
    "        actual_values = df.loc[:, attribute].value_counts().index.values\n",
    "        actual_values = [to_float(x) for x in actual_values]\n",
    "        #diff = set(actual_values) - set(theoretical_vals)\n",
    "        diff = np.setdiff1d(actual_values, theoretical_vals)\n",
    "        if diff.size > 0:\n",
    "            print('Attribute {} has undocumented values {}'.format(attribute, diff))\n",
    "\n",
    "            \n",
    "check_all_values(azdias, feat_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we have:\n",
    "\n",
    "CAMEO_DEU_2015: most common building-type within the cell => XX is a null\n",
    "\n",
    "KBA05_MODTEMP: development of the most common car segment in the neighbourhood: we keep 6.0 although it's not documented\n",
    "\n",
    "LP_FAMILIE_FEIN: familytyp fine: 0 is a null, must be between 1 to 11\n",
    "\n",
    "LP_FAMILIE_GROB familytyp rough, must be 1 to 11, so 0 is a null\n",
    "\n",
    "LP_LEBENSPHASE_FEIN: lifestage fine, must be 1 to 40, so 0 is a null\n",
    "\n",
    "LP_LEBENSPHASE_GROB: lifestage rough, must be 1 to 12, so 0 is  a null\n",
    "\n",
    "ORTSGR_KLS9: size of the community, 1 to 9, so 0 is a null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can fix those above\n",
    "def fix_undocumented_nulls(df):\n",
    "    dxx = {'XX':np.nan}\n",
    "    d0 = {0:np.nan}\n",
    "    df['CAMEO_DEU_2015'] = df['CAMEO_DEU_2015'].replace(dxx)\n",
    "    df['LP_FAMILIE_FEIN'] = df['LP_FAMILIE_FEIN'].replace(d0)\n",
    "    df['LP_FAMILIE_GROB'] = df['LP_FAMILIE_GROB'].replace(d0)\n",
    "    df['LP_LEBENSPHASE_FEIN'] = df['LP_LEBENSPHASE_FEIN'].replace(d0)\n",
    "    df['LP_LEBENSPHASE_GROB'] = df['LP_LEBENSPHASE_GROB'].replace(d0)\n",
    "    df['ORTSGR_KLS9'] = df['ORTSGR_KLS9'].replace(d0)\n",
    "    return df\n",
    "\n",
    "azdias =  fix_undocumented_nulls(azdias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the dataframe we built with the nformation of whoch values cam be considered null to replace the unlknown values with null.\n",
    "We leave the features that have a value equal to zero with Meaning \"no transaction known\"as they represent transactional activity over a period, so we interpret is as \"zero transactions\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's convert the other nulls\n",
    "import progressbar\n",
    "\n",
    "def unknown2nulls(df, df_info):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    n_iters = df_info.shape[0]\n",
    "    cnter = 0\n",
    "    bar = progressbar.ProgressBar(maxval=n_iters+1, widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "    bar.start()\n",
    "    \n",
    "    for _, info_row in df_info[df_info.Value.notnull()].iterrows():\n",
    "        #print('Replacing ', info_row.Attribute)\n",
    "        if isinstance(info_row.Value, str):\n",
    "            d = {}\n",
    "            for unknown_value in info_row.Value.split(','):\n",
    "                d[float(unknown_value)] = np.nan\n",
    "            df[info_row.Attribute] = df[info_row.Attribute].replace(d)\n",
    "        else:\n",
    "            d = {info_row.Value : np.nan}\n",
    "            df[info_row.Attribute] = df[info_row.Attribute].replace(d)\n",
    "        cnter+=1 \n",
    "        bar.update(cnter)\n",
    "    \n",
    "    bar.finish()            \n",
    "    return df\n",
    "\n",
    "azdias = unknown2nulls(azdias, azdias_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing columns to drop\n",
    "We will analyze the amount of missing data and choose which columns to drop.\n",
    "Additionally some columns may be dropped for tyher reasons, for example:\n",
    "- They have all unique values (useless for machine learning)\n",
    "- Categorical features with high cardinality, for example more than 10-15 categories \n",
    "- when we don't have enough information to decide how to encode them\n",
    "- features that are a possible source of ethical bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess missing data in columns\n",
    "plt.hist(azdias.isnull().sum(), bins=20)\n",
    "plt.title('Distributions of null values in the Demographics')\n",
    "plt.xlabel('# Null Values')\n",
    "plt.ylabel('Columns');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution above shows that some outlier features have more than 250,000 null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate patterns in the amount of missing data in each column.\n",
    "columns = azdias.isnull().sum().sort_values(ascending=False)\n",
    "percent_missing = columns/azdias.shape[0]\n",
    "data = percent_missing[percent_missing > 0.3]\n",
    "plt.figure(figsize=(5,15))\n",
    "sns.barplot(y=data.index, x=data.values, orient='h', color='blue')\n",
    "plt.title(\"Attributes with more than 30% of missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now extract a list of the columns to drop. The script will be later run on the customer data set as well, but we won't recalculate the list for the customer dataset. The idea is to use the general population dataset to \"fit\" our methods, like we will do later on with PCA, KMeans, etc, and then apply the fitted methods to tranform the features of the customer or other datasest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The columns to drop should be the same on population and customer dataset, so we leave it outside the function\n",
    "columns_to_drop = [s for s, v in (azdias.isnull().sum() > azdias.shape[0] * 0.3).items() if v]\n",
    "print(columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    columns_to_drop = ['AGER_TYP', 'ALTER_HH', 'ALTER_KIND1', 'ALTER_KIND2',\n",
    "       'ALTER_KIND3', 'ALTER_KIND4', 'D19_BANKEN_ANZ_12',\n",
    "       'D19_BANKEN_ANZ_24', 'D19_BANKEN_DATUM', 'D19_BANKEN_DIREKT',\n",
    "       'D19_BANKEN_GROSS', 'D19_BANKEN_LOKAL', 'D19_BANKEN_OFFLINE_DATUM',\n",
    "       'D19_BANKEN_ONLINE_DATUM', 'D19_BANKEN_REST', 'D19_BEKLEIDUNG_GEH',\n",
    "       'D19_BEKLEIDUNG_REST', 'D19_BILDUNG', 'D19_BIO_OEKO',\n",
    "       'D19_BUCH_CD', 'D19_DIGIT_SERV', 'D19_DROGERIEARTIKEL',\n",
    "       'D19_ENERGIE', 'D19_FREIZEIT', 'D19_GARTEN', 'D19_GESAMT_ANZ_12',\n",
    "       'D19_GESAMT_ANZ_24', 'D19_GESAMT_DATUM',\n",
    "       'D19_GESAMT_OFFLINE_DATUM', 'D19_GESAMT_ONLINE_DATUM',\n",
    "       'D19_HANDWERK', 'D19_HAUS_DEKO', 'D19_KINDERARTIKEL',\n",
    "       'D19_KONSUMTYP', 'D19_KOSMETIK', 'D19_LEBENSMITTEL', 'D19_LOTTO',\n",
    "       'D19_NAHRUNGSERGAENZUNG', 'D19_RATGEBER', 'D19_REISEN',\n",
    "       'D19_SAMMELARTIKEL', 'D19_SCHUHE', 'D19_SONSTIGE', 'D19_SOZIALES',\n",
    "       'D19_TECHNIK', 'D19_TELKO_ANZ_12', 'D19_TELKO_ANZ_24',\n",
    "       'D19_TELKO_DATUM', 'D19_TELKO_MOBILE', 'D19_TELKO_OFFLINE_DATUM',\n",
    "       'D19_TELKO_ONLINE_DATUM', 'D19_TELKO_REST', 'D19_TIERARTIKEL',\n",
    "       'D19_VERSAND_ANZ_12', 'D19_VERSAND_ANZ_24', 'D19_VERSAND_DATUM',\n",
    "       'D19_VERSAND_OFFLINE_DATUM', 'D19_VERSAND_ONLINE_DATUM',\n",
    "       'D19_VERSAND_REST', 'D19_VERSI_ANZ_12', 'D19_VERSI_ANZ_24',\n",
    "       'D19_VERSI_DATUM', 'D19_VERSI_OFFLINE_DATUM',\n",
    "       'D19_VERSI_ONLINE_DATUM', 'D19_VERSICHERUNGEN',\n",
    "       'D19_VOLLSORTIMENT', 'D19_WEIN_FEINKOST', 'EXTSEL992',\n",
    "       'GEBURTSJAHR', 'KK_KUNDENTYP', 'TITEL_KZ']\n",
    "    df = df.drop(columns_to_drop, axis=1)\n",
    "    print(\"Dropped {} columns.\".format(len(columns_to_drop)))\n",
    "    \n",
    "    return df\n",
    "\n",
    "azdias = drop_columns(azdias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column with unique values (very high cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with unique values, doesn't give any information\n",
    "azdias.LNR.value_counts().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we will drop it\n",
    "#azdias.drop('LNR', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features that are a possible source of ethical bias: NATIONALITAET_KZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NATIONALITAET_KZ: possible source of ethical bias\n",
    "#azdias.drop('NATIONALITAET_KZ', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALTERSKATEGORIE_FEIN and ALTERSKATEGORIE_GROB (age through prename analysis have different number of nulls, \n",
    "# in particular ALTERSKATEGORIE_FEIN has 262947 nulls so we prefer to drop it and keep the summary information)\n",
    "#azdias.drop('ALTERSKATEGORIE_FEIN', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for this variable \"type of building (residential or commercial)\" is unbalanced between general population \n",
    "# and customers: The gen pop has only one commercial building (??) and customers has no residential building\n",
    "# So we choose to frop it\n",
    "azdias.GEBAEUDETYP.value_counts(), customers.GEBAEUDETYP.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We put all together in a function\n",
    "\n",
    "def drop_other_columns(df):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    other_columns_to_drop = ['LNR', 'NATIONALITAET_KZ', 'ALTERSKATEGORIE_FEIN', 'GEBAEUDETYP']\n",
    "    df = df.drop(other_columns_to_drop, axis=1)\n",
    "    return df\n",
    "\n",
    "azdias = drop_other_columns(azdias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring Categiorical Features with Multi-values, look for high cardnality features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the other categorical features (the ones who are left)\n",
    "\n",
    "categorical_features = np.intersect1d(categorical_features, azdias.columns.values)\n",
    "\n",
    "categorical_to_drop = []\n",
    "for categ in categorical_features:\n",
    "    num_categs = azdias[categ].value_counts().shape[0]\n",
    "    if num_categs > 15:\n",
    "        print(\"Feature {} has {} categories\".format(categ, num_categs))\n",
    "        categorical_to_drop.append(categ)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will drop CAMEO_DEU_2015 (redundant), EINGEFUEGT_AM (has thousands of dates, not interesting), D19_LETZTER_KAUF_BRANCHE, VERDICHTUNGSRAUM, while it looks  CAMEO_DEUG_2015: CAMEO classification 2015 - Uppergroup carris important information we want to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_categorical(df):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    categorical_to_drop = ['CAMEO_DEU_2015','D19_LETZTER_KAUF_BRANCHE','EINGEFUEGT_AM','VERDICHTUNGSRAUM']\n",
    "    df = df.drop(categorical_to_drop, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "azdias = drop_categorical(azdias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data in rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much data is missing in each row of the dataset?\n",
    "plt.hist(azdias.isnull().sum(axis=1), bins=40)\n",
    "plt.title('Distributions of null values in the rows Demographics')\n",
    "plt.xlabel('# Null Values')\n",
    "plt.ylabel('Rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that the large majority of the population has less than 50 null fatures.\n",
    "# we can remove the datapoints with more than 50 nulls in each row\n",
    "def drop_rows(df, nulls=50):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    df = df.dropna(thresh=df.shape[1]-nulls)\n",
    "    df = df.reset_index()\n",
    "    del df['index']\n",
    "    return df\n",
    "\n",
    "azdias = drop_rows(azdias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can check again....\n",
    "plt.hist(azdias.isnull().sum(axis=1), bins=40)\n",
    "plt.title('Distributions of null values in the rows Demographics')\n",
    "plt.xlabel('# Null Values')\n",
    "plt.ylabel('Rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many rows, columns we are left with\n",
    "azdias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Re-encoding\n",
    "Some categorical features need re-encoding. We will look at the spreadsheet to choose which ones to re-encode.\n",
    "Since the unsupervised learning techniques to be used will only work on data that is encoded numerically, we need to make a few encoding changes or additional assumptions to be able to make progress. In addition, while almost all of the values in the dataset are encoded using numbers, not all of them represent numeric values.\n",
    "\n",
    "- For numeric and interval data, these features can be kept without changes.\n",
    "- Most of the variables in the dataset are ordinal in nature. While ordinal values may technically be non-linear in spacing, we make the simplifying assumption that the ordinal variables can be treated as being interval in nature (that is, kept without any changes).\n",
    "- Special handling will be necessary for the remaining two variable types: categorical, and 'mixed'.\n",
    "\n",
    "Are there binary categorical features that need re-encoding?\n",
    "Are there multi-level categorical features that need re-encoding?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary categorical features\n",
    "# OST_WEST_KZ: flag indicating the former GDR/FRG\n",
    "#azdias['OST_WEST_KZ'].value_counts()\n",
    "def fix_binary_features(df):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    ost_west_dict = {'OST_WEST_KZ': {'W':0, 'O':1}}\n",
    "    df = df.replace(ost_west_dict)\n",
    "    return(df)\n",
    "\n",
    "azdias = fix_binary_features(azdias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBSOLETE: Multi-level categoricals (three or more values) will be one-hot encoded using multiple dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi value features\n",
    "\n",
    "categorical_features = np.intersect1d(azdias_info[azdias_info.Type=='Categorical'].Attribute.values, azdias.columns.values)\n",
    "azdias_info[azdias_info.Attribute.isin(categorical_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.LP_FAMILIE_GROB.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBSOLETE: Actually one-hot encoding doesn't work well with Tree-based methods liked Random Forest or Gradient Boost, \n",
    "# so we remove it for now\n",
    "# We drop LP_FAMILIE_GROB and one-hot encode the other categorical feats\n",
    "\n",
    "one_hot_encode_cols = np.intersect1d(azdias_info[azdias_info.Type=='Categorical'].Attribute.values, azdias.columns.values)\n",
    "one_hot_encode_cols = np.setdiff1d(one_hot_encode_cols, ['LP_FAMILIE_GROB'])\n",
    "\n",
    "print(one_hot_encode_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical(df, azdias_info):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    one_hot_encode_cols = ['CAMEO_DEUG_2015', 'CJT_GESAMTTYP', 'FINANZTYP',\n",
    "       'GEBAEUDETYP_RASTER', 'GEMEINDETYP', 'GFK_URLAUBERTYP',\n",
    "       'HEALTH_TYP', 'LP_FAMILIE_FEIN', 'LP_LEBENSPHASE_FEIN',\n",
    "       'LP_LEBENSPHASE_GROB', 'REGIOTYP', 'RETOURTYP_BK_S', 'SHOPPER_TYP',\n",
    "       'VK_DHT4A', 'WOHNLAGE', 'ZABEOTYP']\n",
    "    \n",
    "    #df = df.drop('LP_FAMILIE_GROB', axis=1)\n",
    "    #df = pd.get_dummies(df, columns=one_hot_encode_cols)\n",
    "    \n",
    "    return df\n",
    "\n",
    "#azdias = encode_categorical(azdias, azdias_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineer Mixed-Type Features\n",
    "There are two of features that we marked as \"mixed\" in the azdias_info dataframe that require special treatment in order to be included in the analysis. \n",
    "- \"PRAEGENDE_JUGENDJAHRE\" combines information on three dimensions: generation by decade, movement (mainstream vs. avantgarde), and nation (east vs. west). While there aren't enough levels to disentangle east from west, you should create two new variables to capture the other two dimensions: an interval-type variable for decade, and a binary variable for movement.\n",
    "- \"CAMEO_INTL_2015\" combines information on two axes: wealth and life stage. Break up the two-digit codes by their 'tens'-place and 'ones'-place digits into two new ordinal variables (which, for the purposes of this project, is equivalent to just treating them as their raw numeric values).\n",
    "- WOHNLAGE (Residential Area): combines neighboorhood quality and rural/non-rural information. We create two new features NEIGHBORHOOD_QUALITY (Ordinal, vaulues from 1 to 5) and RURAL (Binary, 0= non-rural, 1=rural)\n",
    "- LP_LEBENSPHASE_FEIN has information over 3 dimensions: Income (from low-income to top-income), Age (younger/middle/older/advanced/retirement) and Family Status (single/couples/families/multiperson households). However his contents are summarized in the LP_LEBENSPHASE_GROB feature that is much easier to interpret, with values from 1 (single low-income) to 12 (high-income earners of higher age from multiperson households). So we keep this last one and we will drop LP_LEBENSPHASE_FEIN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_info[azdias_info.Type == 'Mix-Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.LP_LEBENSPHASE_FEIN.isnull().sum(), azdias.LP_LEBENSPHASE_GROB.isnull().sum()\n",
    "azdias.LP_LEBENSPHASE_GROB.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rural_dict = {1.0:0, 2.0:0, 3.0:0, 4.0:0, 5.0:0, 7.0:1, 8.0:1}\n",
    "#azdias.WOHNLAGE.value_counts()\n",
    "azdias['NEIGHBORHOOD_QUALITY'] = np.nan\n",
    "azdias['NEIGHBORHOOD_QUALITY']  = azdias[(azdias['WOHNLAGE'] > 0) & (azdias['WOHNLAGE'] < 7)]['WOHNLAGE']\n",
    "azdias['NEIGHBORHOOD_QUALITY'].value_counts()\n",
    "azdias['RURAL'] = np.nan\n",
    "azdias['RURAL'] = azdias['WOHNLAGE'].map(rural_dict)\n",
    "azdias.RURAL.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate \"PRAEGENDE_JUGENDJAHRE\" and engineer two new variables.\n",
    "azdias['PRAEGENDE_JUGENDJAHRE'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WOHNLAGE\n",
    "Residential Area\n",
    "- -1\tunknown\n",
    "-\t0\tno score calculated\n",
    "-\t1\tvery good neighbourhood\n",
    "-\t2\tgood neighbourhood\n",
    "-\t3\taverage neighbourhood\n",
    "-\t4\tpoor neighbourhood\n",
    "-\t5\tvery poor neighbourhood\n",
    "-\t7\trural neighbourhood\n",
    "-\t8\tnew building in rural neighbourhood\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PRAEGENDE_JUGENDJAHRE\n",
    "Dominating movement of person's youth (avantgarde vs. mainstream; east vs. west)\n",
    "- -1: unknown\n",
    "-  0: unknown\n",
    "-  1: 40s - war years (Mainstream, E+W)\n",
    "-  2: 40s - reconstruction years (Avantgarde, E+W)\n",
    "-  3: 50s - economic miracle (Mainstream, E+W)\n",
    "-  4: 50s - milk bar / Individualisation (Avantgarde, E+W)\n",
    "-  5: 60s - economic miracle (Mainstream, E+W)\n",
    "-  6: 60s - generation 68 / student protestors (Avantgarde, W)\n",
    "-  7: 60s - opponents to the building of the Wall (Avantgarde, E)\n",
    "-  8: 70s - family orientation (Mainstream, E+W)\n",
    "-  9: 70s - peace movement (Avantgarde, E+W)\n",
    "- 10: 80s - Generation Golf (Mainstream, W)\n",
    "- 11: 80s - ecological awareness (Avantgarde, W)\n",
    "- 12: 80s - FDJ / communist party youth organisation (Mainstream, E)\n",
    "- 13: 80s - Swords into ploughshares (Avantgarde, E)\n",
    "- 14: 90s - digital media kids (Mainstream, E+W)\n",
    "- 15: 90s - ecological awareness (Avantgarde, E+W)\n",
    "\n",
    "\n",
    "German CAMEO: Wealth / Life Stage Typology, mapped to international code\n",
    "- -1: unknown\n",
    "- 11: Wealthy Households - Pre-Family Couples & Singles\n",
    "- 12: Wealthy Households - Young Couples With Children\n",
    "- 13: Wealthy Households - Families With School Age Children\n",
    "- 14: Wealthy Households - Older Families &  Mature Couples\n",
    "- 15: Wealthy Households - Elders In Retirement\n",
    "- 21: Prosperous Households - Pre-Family Couples & Singles\n",
    "- 22: Prosperous Households - Young Couples With Children\n",
    "- 23: Prosperous Households - Families With School Age Children\n",
    "- 24: Prosperous Households - Older Families & Mature Couples\n",
    "- 25: Prosperous Households - Elders In Retirement\n",
    "- 31: Comfortable Households - Pre-Family Couples & Singles\n",
    "- 32: Comfortable Households - Young Couples With Children\n",
    "- 33: Comfortable Households - Families With School Age Children\n",
    "- 34: Comfortable Households - Older Families & Mature Couples\n",
    "- 35: Comfortable Households - Elders In Retirement\n",
    "- 41: Less Affluent Households - Pre-Family Couples & Singles\n",
    "- 42: Less Affluent Households - Young Couples With Children\n",
    "- 43: Less Affluent Households - Families With School Age Children\n",
    "- 44: Less Affluent Households - Older Families & Mature Couples\n",
    "- 45: Less Affluent Households - Elders In Retirement\n",
    "- 51: Poorer Households - Pre-Family Couples & Singles\n",
    "- 52: Poorer Households - Young Couples With Children\n",
    "- 53: Poorer Households - Families With School Age Children\n",
    "- 54: Poorer Households - Older Families & Mature Couples\n",
    "- 55: Poorer Households - Elders In Retirement\n",
    "- XX: unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-engineer variables\n",
    "def engineer_variables(df):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # MAINSTREAM AND GENERATION\n",
    "    mainstream_dict = {1.0:1, 2.0:2, 3.0:1, 4.0:2, 5.0:1, 6.0:2, 7.0:2, 8.0:1, 9.0:2, 10.0:1, 11.0:2, 12.0:1, 13.0:2, 14.0:1, 15.0:2}\n",
    "    df['MAINSTREAM'] = np.nan\n",
    "    df['MAINSTREAM'] = df['PRAEGENDE_JUGENDJAHRE'].map(mainstream_dict)\n",
    "    generation_dict = {1.0:1, 2.0:1, 3.0:2, 4.0:2, 5.0:3, 6.0:3, 7.0:3, 8.0:4, 9.0:4, 10.0:5, 11.0:5, 12.0:5, 13.0:5, 14.0:6, 15.0:6}\n",
    "    df['GENERATION'] = np.nan\n",
    "    df['GENERATION'] = df['PRAEGENDE_JUGENDJAHRE'].map(generation_dict)  \n",
    "    df['WEALTH'] = df['CAMEO_INTL_2015'].apply(lambda x: np.floor_divide(float(x), 10) if float(x) else np.nan)\n",
    "    df['FAMILY'] = df['CAMEO_INTL_2015'].apply(lambda x: np.mod(float(x), 10) if float(x) else np.nan) \n",
    "    \n",
    "    rural_dict = {1.0:0, 2.0:0, 3.0:0, 4.0:0, 5.0:0, 7.0:1, 8.0:1}\n",
    "    df['NEIGHBORHOOD_QUALITY'] = np.nan\n",
    "    df['NEIGHBORHOOD_QUALITY']  = df[(df['WOHNLAGE'] > 0) & (df['WOHNLAGE'] < 7)]['WOHNLAGE']\n",
    "    df['NEIGHBORHOOD_QUALITY'].value_counts()\n",
    "    df['RURAL'] = np.nan\n",
    "    df['RURAL'] = df['WOHNLAGE'].map(rural_dict)\n",
    "\n",
    "    # Remove old variables\n",
    "    df = df.drop(['PRAEGENDE_JUGENDJAHRE', 'CAMEO_INTL_2015', 'WOHNLAGE', 'LP_LEBENSPHASE_FEIN'],axis=1)\n",
    "    return df\n",
    "\n",
    "azdias = engineer_variables(azdias)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have decided not to one-hot encode categorical variables as it does not help when using Tree-based methods like Decisopn Trees, RandomForest or GradientBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will put all the cleaning into one function, so that we can apply it to the customer database later on\n",
    "def clean_data(df, azdias_info, cross_validation=False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    print(\"Converting unknown to nulls...\")\n",
    "    df = fix_cameo(df)\n",
    "    df = fix_undocumented_nulls(df)\n",
    "    df = unknown2nulls(df, azdias_info)\n",
    "    print(\"Dropping columns...\")\n",
    "    df = drop_columns(df)\n",
    "    df = drop_other_columns(df)\n",
    "    df = drop_categorical(df)\n",
    "    if cross_validation == False:\n",
    "        print(\"Dropping rows...\")\n",
    "        df = drop_rows(df)\n",
    "    df = fix_binary_features(df)\n",
    "    # We don't one-hot encode as it doesn't help with Tree based methods we intend to use later on\n",
    "    #print(\"One hot encoding...\")\n",
    "    #df = encode_categorical(df, azdias_info)\n",
    "    print(\"Engineering new variables...\")\n",
    "    df = engineer_variables(df)\n",
    "    print(\"Done.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "#azdias = clean_data(azdias, azdias_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.to_feather('azdias_cleaned.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without one-hot encoding\n",
    "cat_cols = np.intersect1d(azdias_info[azdias_info.Type=='Categorical'].Attribute.values, azdias.columns.values)\n",
    "num_cols = np.intersect1d(azdias_info[azdias_info.Type=='Numeric'].Attribute.values, azdias.columns.values)\n",
    "ord_cols = np.intersect1d(azdias_info[azdias_info.Type=='Ordinal'].Attribute.values, azdias.columns.values)\n",
    "mix_cols = np.intersect1d(azdias_info[azdias_info.Type=='Mix-Type'].Attribute.values, azdias.columns.values)\n",
    "bin_cols = np.intersect1d(azdias_info[azdias_info.Type=='Binary'].Attribute.values, azdias.columns.values)\n",
    "bin_cols = np.append(bin_cols, 'MAINSTREAM')\n",
    "ord_cols = np.concatenate((ord_cols, ['GENERATION', 'WEALTH', 'FAMILY', 'NEIGHBORHOOD_QUALITY', 'RURAL']))\n",
    "print(len(cat_cols))\n",
    "print(len(num_cols))\n",
    "print(len(ord_cols))\n",
    "print(len(mix_cols))\n",
    "print(len(bin_cols))\n",
    "azdias.shape, len(cat_cols)+len(num_cols)+len(ord_cols)+len(mix_cols)+len(bin_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformation\n",
    "\n",
    "### Apply Feature Scaling\n",
    "\n",
    "Before we apply dimensionality reduction techniques to the data, we need to perform feature scaling so that the principal component vectors are not influenced by the natural differences in scale for features.  In this substep, we will  check the following:\n",
    "\n",
    "- Before applying the scaler to the data, we need to make sure that we've cleaned the DataFrame of the remaining missing values. This could be as simple as just removing all data points with missing data, thereby losing a significative percentage of the data, or applying an [Imputer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html) to replace all missing values. \n",
    "- We will separate categorical variables from ordinal and numeric ones, as we will use two different strategies for imputing. Then we intersect with the columns that have null values and finally we add the new engineered Mixed-Type Features\n",
    "- When imputing, the biggest concern is introducing bias into the data, therefore it's advisable to use different strategies depending on the type of the data. We will use most frequent values for binary variable, median for ordinal ones, and categorical variables have been already handled by creating dummy columns. Finally for numeric columns, if they are skewed we will apply a logarithmic tranform, then impute the median.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias = pd.read_feather('azdias_cleaned.feather', nthreads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.shape\n",
    "#azdias.columns[azdias.isnull().any()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d19_cols = [col for col in azdias if col.startswith('D19')]\n",
    "for d19 in d19_cols:\n",
    "    print(\"Column {} has {} nulls\".format(d19, azdias[d19].isnull().sum()))\n",
    "\n",
    "d19_features = ['D19_BANKEN_ONLINE_QUOTE_12', 'D19_GESAMT_ONLINE_QUOTE_12', 'D19_TELKO_ONLINE_QUOTE_12', 'D19_VERSAND_ONLINE_QUOTE_12', 'D19_VERSI_ONLINE_QUOTE_12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.columns[azdias.isnull().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBSOLETE\n",
    "\n",
    "# Now for the other columns we need to know which are numerical, ordinal, categorical. \n",
    "#And we shoudn't forget the last 4 column we added\n",
    "columns_with_nulls = azdias.columns[azdias.isnull().any()]\n",
    "# The columns we added manually are the last 4, plus KBA05_GBZ' that need to be added to the list\n",
    "# because it has nulls in the customer dataset, otherwise the pca transform will fail\n",
    "\n",
    "cols_to_impute = np.setdiff1d(columns_with_nulls, d19_impute_zero)\n",
    "\n",
    "numeric2impute = azdias_info[(azdias_info.Attribute.isin(cols_to_impute)) & (azdias_info.Type =='Numeric')].Attribute.values\n",
    "categorical2impute = azdias_info[(azdias_info.Attribute.isin(cols_to_impute)) & (azdias_info.Type =='Categorical')].Attribute.values\n",
    "ordinal2impute = azdias_info[(azdias_info.Attribute.isin(cols_to_impute)) & (azdias_info.Type =='Ordinal')].Attribute.values\n",
    "binary2impute = azdias_info[(azdias_info.Attribute.isin(cols_to_impute)) & (azdias_info.Type =='Binary')].Attribute.values\n",
    "binary2impute = np.append(binary2impute, 'MAINSTREAM')\n",
    "ordinal2impute = np.concatenate((ordinal2impute, ['GENERATION', 'WEALTH', 'FAMILY', 'KBA05_GBZ']))\n",
    "categorical2impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.LP_LEBENSPHASE_GROB.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Pipeline\n",
    "We have a problem with the D19_ that have a very high number of null values. \n",
    "Imputing with a median would introduce an unacceptable bias as the second most frequent value is usually in the few thousands.\n",
    "Therefor we will:\n",
    "- impute a zero in the D19_.._QUOTE12 variables (equivalent to \"no Online-transactions within the last 12 months\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d19_cols = [col for col in azdias if col.startswith('D19')]\n",
    "for d19 in d19_cols:\n",
    "    print(\"Column {} has {} nulls\".format(d19, azdias[d19].isnull().sum()))\n",
    "\n",
    "d19_features = ['D19_BANKEN_ONLINE_QUOTE_12', 'D19_GESAMT_ONLINE_QUOTE_12', 'D19_TELKO_ONLINE_QUOTE_12', 'D19_VERSAND_ONLINE_QUOTE_12', 'D19_VERSI_ONLINE_QUOTE_12']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approach: we fit the imputer to all columns instead of only columns with nulls, so to have more flexibility \n",
    "# when applying the imputer to the customer or other datasets\n",
    "\n",
    "ordinal_features = np.intersect1d(azdias_info[azdias_info.Type == 'Ordinal'].Attribute.values, azdias.columns.values)\n",
    "ordinal_features = np.setdiff1d(ordinal_features, d19_features)\n",
    "# This one should be empty because we one-hot-encoded them\n",
    "categorical_features = np.intersect1d(azdias_info[azdias_info.Type == 'Categorical'].Attribute.values, azdias.columns.values)\n",
    "numeric_features = np.intersect1d(azdias_info[azdias_info.Type == 'Numeric'].Attribute.values, azdias.columns.values)\n",
    "binary_features =   np.intersect1d(azdias_info[azdias_info.Type == 'Binary'].Attribute.values, azdias.columns.values)\n",
    "# Mix-Type\n",
    "mix_type_features = np.intersect1d(azdias_info[azdias_info.Type == 'Mix-Type'].Attribute.values, azdias.columns.values)\n",
    "\n",
    "# Add Re-engineerd variables\n",
    "binary_features = np.append(binary_features, 'MAINSTREAM')\n",
    "ordinal_features = np.concatenate((ordinal_features, ['GENERATION', 'WEALTH', 'FAMILY',  'NEIGHBORHOOD_QUALITY', 'RURAL']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias[list(numeric_features)+list(binary_features)+list(ordinal_features)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "def log_adjusted(x):\n",
    "    return np.log1p(1+x)\n",
    "\n",
    "def inv_log_adjusted(x):\n",
    "    return np.expm1(x)-1\n",
    "   \n",
    "\n",
    "log_transform = ('log_transform', FunctionTransformer(func = log_adjusted, inverse_func=inv_log_adjusted, validate=False))\n",
    "log_impute = ('log_impute', SimpleImputer(missing_values=np.nan, strategy='median'))\n",
    "log_scale = ('log_scale', StandardScaler())\n",
    "\n",
    "log_pipeline = Pipeline([log_transform, log_impute, log_scale])\n",
    "\n",
    "binary_pipeline = Pipeline([('binary_impute', SimpleImputer(missing_values=np.nan, strategy='most_frequent'))])\n",
    "\n",
    "#cat_pipeline = Pipeline([('cat_dummy', etl.DummiesTransformer())])\n",
    "cat_pipeline = Pipeline([('cat_impute', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)),\n",
    "                          ('cat_scale', StandardScaler())])\n",
    "ordinal_impute = ('ordinal_impute', SimpleImputer(missing_values=np.nan, strategy='median'))\n",
    "ordinal_scale = ('ordinal_scale', StandardScaler())\n",
    "\n",
    "ordinal_pipeline = Pipeline([ordinal_impute, ordinal_scale])\n",
    "\n",
    "d19_pipeline = Pipeline([('d19_impute', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)),\n",
    "                          ('d19_scale', StandardScaler())])\n",
    "\n",
    "mix_pipeline = Pipeline([('mix_impute', SimpleImputer(missing_values=np.nan, strategy='median')),\n",
    "                          ('mix_scale', StandardScaler())])\n",
    "\n",
    "transformers = [('log', log_pipeline, numeric_features),\n",
    "                ('binary', binary_pipeline, binary_features),\n",
    "                ('cat', cat_pipeline, categorical_features),\n",
    "                ('ordinal', ordinal_pipeline, ordinal_features),\n",
    "                ('mix', mix_pipeline, mix_type_features),\n",
    "                ('d19', d19_pipeline, d19_features)]\n",
    "\n",
    "c_transformer = ColumnTransformer(transformers=transformers)\n",
    "\n",
    "\n",
    "\n",
    "#columns_ex_cat = list(numeric_features)+list(binary_features)+list(ordinal_features) \n",
    "azdias_transformed = c_transformer.fit_transform(azdias)\n",
    "\n",
    "azdias_transformed.shape, azdias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can rebuild the DataFrame if we need\n",
    "azdias_columns = list(numeric_features)+list(binary_features)+list(categorical_features)+list(ordinal_features)+list(mix_type_features)+list(d19_features)\n",
    "azdias_transformed = pd.DataFrame(azdias_transformed, columns=azdias_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now have a look at the distributions of numerical variables\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.hist(azdias.ANZ_HAUSHALTE_AKTIV.dropna());\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.hist(azdias.ANZ_HH_TITEL.dropna());\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.hist(azdias.ANZ_PERSONEN.dropna());\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.hist(azdias.ANZ_KINDER.dropna());\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.hist(azdias.ANZ_TITEL.dropna());\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.hist(azdias.ANZ_STATISTISCHE_HAUSHALTE.dropna());\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The therendistributiones look all right skewed: when data are very non-normal (and e.g. needs a log-transformation to give approximate normality), then any imputation method assuming normality may not perform so well. Therefore we will log-tranform the numerical features before applying imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transform numeric features\n",
    "#from sklearn.preprocessing import FunctionTransformer\n",
    "def log_transform_numeric(df):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    df['ANZ_HAUSHALTE_AKTIV'] = df['ANZ_HAUSHALTE_AKTIV'].apply(lambda x: np.log1p(1+x) if x != np.nan else x)\n",
    "    df['ANZ_HH_TITEL'] = df['ANZ_HH_TITEL'].apply(lambda x: np.log1p(1+x) if x != np.nan else x)\n",
    "    df['ANZ_PERSONEN'] = df['ANZ_PERSONEN'].apply(lambda x: np.log1p(1+x) if x != np.nan else x)\n",
    "    df['ANZ_TITEL'] = df['ANZ_TITEL'].apply(lambda x: np.log1p(1+x) if x != np.nan else x)\n",
    "    df['ANZ_KINDER'] = df['ANZ_KINDER'].apply(lambda x: np.log1p(1+x) if x != np.nan else x)\n",
    "    df['KBA13_ANZAHL_PKW'] = df['KBA13_ANZAHL_PKW'].apply(lambda x: np.log1p(1+x) if x != np.nan else x)\n",
    "    df['ANZ_STATISTISCHE_HAUSHALTE'] = df['ANZ_STATISTISCHE_HAUSHALTE'].apply(lambda x: np.log1p(1+x) if x != np.nan else x)\n",
    "    \n",
    "    return df\n",
    "\n",
    "azdias = log_transform_numeric(azdias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can have a look at the distributions of numerical variables again\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.hist(azdias.ANZ_HAUSHALTE_AKTIV.dropna());\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.hist(azdias.ANZ_HH_TITEL.dropna());\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.hist(azdias.ANZ_PERSONEN.dropna());\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.hist(azdias.ANZ_KINDER.dropna());\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.hist(azdias.ANZ_TITEL.dropna());\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.hist(azdias.ANZ_STATISTISCHE_HAUSHALTE.dropna());\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit better, but still skewed. So we will use meadian to impute instead of mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBSOLETE\n",
    "#Doesn't work well like this: we need a fit and a transform method, like in a pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "def impute_fit_transform(df):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    imputer_ordinal = SimpleImputer(strategy='median')\n",
    "    df[ordinal_features] = imputer_ordinal.fit_transform(df[ordinal_features])\n",
    "    # Numeric features\n",
    "    imputer_numeric = SimpleImputer(strategy='median')\n",
    "    df[numeric_features] = imputer_numeric.fit_transform(df[numeric_features])\n",
    "    # Binary features\n",
    "    imputer_binary = SimpleImputer(strategy='most_frequent')\n",
    "    df[binary_features] = imputer_binary.fit_transform(df[binary_features])  \n",
    "    \n",
    "    # Set aside the fitted objects\n",
    "    imputers = [imputer_ordinal, imputer_numeric, imputer_binary]\n",
    "        \n",
    "    # Let's not forget the d19\n",
    "    df[d19_impute_zero] = df[d19_impute_zero].fillna(0)\n",
    "\n",
    "    return df, imputers\n",
    "\n",
    "\n",
    "def impute_transform(df, imputers):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # Extract imputers and transform only\n",
    "    imputer_ordinal = imputers[0]\n",
    "    imputer_numeric = imputers[1]\n",
    "    imputer_binary = imputers[2]\n",
    "    \n",
    "\n",
    "    df[ordinal_features] = imputer_ordinal.transform(df[ordinal_features])\n",
    "    # Numeric features\n",
    "\n",
    "    df[numeric_features] = imputer_numeric.transform(df[numeric_features])\n",
    "    # Binary features\n",
    "\n",
    "    df[binary_features] = imputer_binary.transform(df[binary_features])  \n",
    "        \n",
    "    # Let's not forget the d19\n",
    "    df[d19_impute_zero] = df[d19_impute_zero].fillna(0)\n",
    "\n",
    "    return df\n",
    "\n",
    "azdias, imputers = impute_fit_transform(azdias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBSOLETE\n",
    "# Now we can impute: we need to save the fitted objects to use them with the customer data\n",
    "from sklearn.impute import SimpleImputer\n",
    "def impute_nulls(df, azdias_info, imputers = []):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    #The columns to impute should not be recomputed for the customer dataset again, so we leave it outside the function\n",
    "    \n",
    "    if imputers == []:\n",
    "        # Ordinal features\n",
    "        imputer_ordinal = SimpleImputer(strategy='median')\n",
    "        df[ordinal2impute] = imputer_ordinal.fit_transform(df[ordinal2impute])\n",
    "        # Numeric features\n",
    "        imputer_numeric = SimpleImputer(strategy='median')\n",
    "        df[numeric2impute] = imputer_numeric.fit_transform(df[numeric2impute])\n",
    "        # Binary features\n",
    "        imputer_binary = SimpleImputer(strategy='most_frequent')\n",
    "        df[binary2impute] = imputer_binary.fit_transform(df[binary2impute])  \n",
    "        # Set aside the fitted objects\n",
    "        imputers = [imputer_ordinal, imputer_numeric, imputer_binary]\n",
    "    else:\n",
    "        # Extract imputers and transform only\n",
    "        imputer_ordinal = imputers[0]\n",
    "        imputer_numeric = imputers[1]\n",
    "        imputer_binary = imputers[2]\n",
    "        \n",
    "        df[ordinal2impute] = imputer_ordinal.transform(df[ordinal2impute])\n",
    "        df[numeric2impute] = imputer_numeric.transform(df[numeric2impute])\n",
    "        df[binary2impute] = imputer_binary.transform(df[binary2impute])\n",
    "    \n",
    "    # Let's not forget the d19\n",
    "    df[d19_impute_zero] = df[d19_impute_zero].fillna(0)\n",
    "        \n",
    "    return df, imputers\n",
    "\n",
    "azdias, imputers = impute_nulls(azdias, azdias_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can apply Standard Scaling to all numerical and ordinal columns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#ordinal_features = np.intersect1d(azdias_info[azdias_info.Type=='Ordinal'].Attribute.values, azdias.columns.values)\n",
    "#ordinal_features = np.concatenate((ordinal_features, ['GENERATION', 'WEALTH', 'FAMILY']))\n",
    "#numeric_features = np.intersect1d(azdias_info[azdias_info.Type=='Numeric'].Attribute.values, azdias.columns.values)\n",
    "features_to_scale = np.concatenate((ordinal_features, numeric_features))\n",
    "\n",
    "def scale_features(df, scaler=None):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    if scaler == None:\n",
    "        scaler=StandardScaler()\n",
    "        df[features_to_scale] = scaler.fit_transform(df[features_to_scale])\n",
    "    else:\n",
    "        # Transform only\n",
    "        df[features_to_scale] = scaler.transform(df[features_to_scale])\n",
    "    return df, scaler\n",
    "\n",
    "azdias, scaler = scale_features(azdias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can be put all together in a function\n",
    "def feature_transformation(df, azdias_info, imputers = [], scaler=None):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"Log Transforming numeric features...\")\n",
    "    df = log_transform_numeric(df)\n",
    "    print(\"Imputing missing values...\")\n",
    "    #if imputers == []:\n",
    "    #    df, imputers = impute_fit_transform(df)\n",
    "    #else:\n",
    "    #    df = impute_transform(df, imputers)\n",
    "    df, imputers = impute_nulls(df, azdias_info, imputers)    \n",
    "    print(\"Scaling features...\")\n",
    "    df, scaler = scale_features(df, scaler)\n",
    "    print(\"Done.\")\n",
    "    return df, imputers, scaler\n",
    "\n",
    "#azdias, imputers, scaler = feature_transformation(azdias, azdias_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputers, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.to_feather('azdias_transformed.feather')\n",
    "#azdias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Customer Segmentation Report\n",
    "\n",
    "The main bulk of your analysis will come in this part of the project. Here, you should use unsupervised learning techniques to describe the relationship between the demographics of the company's existing customers and the general population of Germany. By the end of this part, you should be able to describe parts of the general population that are more likely to be part of the mail-order company's main customer base, and which parts of the general population are less so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "On the scaled data, we are now ready to apply dimensionality reduction techniques.\n",
    "\n",
    "- Use sklearn's [PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) class to apply principal component analysis on the data, thus finding the vectors of maximal variance in the data. To start, you should not set any parameters (so all components are computed) or set a number of components that is at least half the number of features (so there's enough features to see the general trend in variability).\n",
    "- Check out the ratio of variance explained by each principal component as well as the cumulative variance explained. Try plotting the cumulative or sequential values using matplotlib's [`plot()`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.plot.html) function. Based on what you find, select a value for the number of transformed features you'll retain for the clustering part of the project.\n",
    "- Once you've made a choice for the number of components to keep, make sure you re-fit a PCA instance to perform the decided-on transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#azdias = pd.read_feather('azdias_scaled.feather', nthreads=2)\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(azdias_transformed)\n",
    "azdias_pca = pca.transform(azdias_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_pca(pca):\n",
    "    cumulative_ratios=np.zeros(len(pca.explained_variance_ratio_))\n",
    "    for i in range(len(pca.explained_variance_ratio_)):\n",
    "        cumulative_ratios[i]=np.sum(pca.explained_variance_ratio_[:i])\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.plot(pca.explained_variance_ratio_)\n",
    "    plt.plot(cumulative_ratios)\n",
    "    plt.xlabel(\"Components\")\n",
    "    plt.ylabel(\"Explained Variance Ratio %\")\n",
    "    plt.title(\"PCA Components Explained Variance Ratios\")\n",
    "    plt.yticks(np.arange(0, 1, step=0.05))\n",
    "    plt.xticks(np.arange(0, len(pca.explained_variance_ratio_)+2, step= (len(pca.explained_variance_ratio_) // 20)))\n",
    "    plt.grid(linewidth=0.1)\n",
    "    plt.legend(['Variance Ratio', 'Cumulative'], loc='center right')\n",
    "    \n",
    "#show_pca(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(pca.explained_variance_ratio_[:140])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pca without parameters returned all the 289 components (same as n. of features)\n",
    "The plot shows that over 80% of the variance is explained by the first 90 components, 90% by the first 140 components and after that it increases very slowly\n",
    "\n",
    "So we will run it again but this time we want only 140 components. Also we are saving the pca object so we can ru it \n",
    "on the customer dataset later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(df, n_components=300):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(df)\n",
    "    df_pca = pca.transform(df)\n",
    "    \n",
    "    return df_pca, pca\n",
    "\n",
    "azdias_pca, pca = apply_pca(azdias_transformed, 140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_pca(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "azdias.to_feather('azdias_pca.feather')\n",
    "\n",
    "# Save to file in the current working directory\n",
    "pkl_filename = \"pca.pkl\"  \n",
    "with open(pkl_filename, 'wb') as file:  \n",
    "    pickle.dump(pca, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_results(full_dataset, pca, component):\n",
    "    '''\n",
    "    Create a DataFrame of the PCA results\n",
    "    Includes dimension feature weights and explained variance\n",
    "    Visualizes the PCA results\n",
    "    '''\n",
    "\n",
    "    # Dimension indexing\n",
    "    dimensions = ['Dimension {}'.format(i) for i in range(1,len(pca.components_)+1)]\n",
    "\n",
    "    # PCA components\n",
    "    components = pd.DataFrame(np.round(pca.components_, 4), columns = full_dataset.keys())\n",
    "    components.index = dimensions\n",
    "\n",
    "    # PCA explained variance\n",
    "    ratios = pca.explained_variance_ratio_.reshape(len(pca.components_), 1)\n",
    "    variance_ratios = pd.DataFrame(np.round(ratios, 4), columns = ['Explained Variance'])\n",
    "    variance_ratios.index = dimensions\n",
    "\n",
    "    # Create a bar plot visualization\n",
    "    fig, ax = plt.subplots(figsize = (20,10))\n",
    "\n",
    "    # Plot the feature weights as a function of the components\n",
    "    features_to_show = components.iloc[component - 1].sort_values(ascending=False)\n",
    "    features_to_show = features_to_show[np.absolute(features_to_show.values) >= 0.01]\n",
    "    #components.iloc[component - 1].sort_values(ascending=False).plot(ax = ax, kind = 'bar');\n",
    "    features_to_show.plot(ax = ax, kind = 'bar');\n",
    "    ax.set_ylabel(\"Feature Weights\")\n",
    "    ax.set_xticklabels(list(features_to_show.keys()), rotation=90)\n",
    "    ax.set_xlabel(\"Features\")\n",
    "\n",
    "    # Display the explained variance ratios\n",
    "    #for i, ev in enumerate(pca.explained_variance_ratio_):\n",
    "    ev = pca.explained_variance_ratio_[component-1]\n",
    "    \n",
    "    plt.title(\"Component {} Explained Variance: {:.2f}%\".format(component, ev*100))\n",
    "\n",
    "    # Return a concatenated DataFrame\n",
    "    #return pd.concat([variance_ratios, components], axis = 1).iloc[component - 1].sort_values(ascending=False)\n",
    "    return features_to_show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_results(azdias_transformed, pca, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive correlations\n",
    "PLZ8_ANTG3: # of 6-10 family houses in the area\n",
    "PLZ8_ANTG4 # of +10 families in the area  \n",
    "KBA13_BAUMAX : Building type\n",
    "ANZ_HAUSHALTE_AKTIV # of households in the building\n",
    "WEALTH: here small value means wealthy households, high values poorer households\n",
    "\n",
    "Neg Correlations:\n",
    "MOBI_REGIO: mobility patterns, where a higher value means low mobility, more stability\n",
    "PLZ_ANTG1: share of small families\n",
    "LP_STATUS_FEIN/GROB: Social status\n",
    "\n",
    "So this component is positively correlated to share of bigger families (more densily populated, less wealthy) in the area and building and negatively correlated with social status and stability (younger, not wealthy tend to rent and change often) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set apart the most important positively and negatively correlated fetures\n",
    "first_component_pos = ['PLZ8_ANTG3','KBA13_ANTG3','KBA13_ANTG4','KBA13_BAUMAX','PLZ8_ANTG4','PLZ8_BAUMAX','ANZ_HAUSHALTE_AKTIV','ANZ_STATISTISCHE_HAUSHALTE','CAMEO_DEUG_2015','WEALTH']\n",
    "first_component_neg = ['LP_STATUS_GROB','MOBI_RASTER','LP_STATUS_FEIN','KBA05_ANTG1','KBA13_ANTG1','PLZ8_ANTG1','MOBI_REGIO']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second component\n",
    "pca_results(azdias_transformed, pca, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive Corr:\n",
    "KBA13_HERST_BMW_BENZ           0.1890: Share of BMW & Mercedes in the area\n",
    "KBA13_SEG_OBEREMITTELKLASSE    0.1621: share of upper middle class cars and upper class cars (BMW5er, BMW7er etc.)\n",
    "KBA13_MERCEDES                 0.1608: share of Mercedes in the area\n",
    "KBA13_BMW : share of BMW in the area\n",
    "KBA13_SEG_SPORTWAGEN: Share of sportcars\n",
    "    \n",
    "Negatice corr:\n",
    "KBA13_SEG_KLEINWAGEN:    Share of small cars\n",
    "KBA13_KMH_140_210 : Share of cars with max speed between 140 and 210\n",
    "KBA13_HALTER_25, KBA13_HALTER_20: Young drivers\n",
    "\n",
    "Looks like this component is positively correlated with uppr-middle class indicators like owning a BMW, Merc or Sportcar, mature drivers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_component_pos = ['KBA13_HERST_BMW_BENZ','KBA13_SEG_OBEREMITTELKLASSE','KBA13_MERCEDES','KBA13_BMW','KBA13_SITZE_4','KBA13_SEG_SPORTWAGEN']\n",
    "second_component_neg = ['KBA13_HALTER_25','KBA13_KMH_180','KBA13_KMH_140_210','KBA13_SEG_KLEINWAGEN','KBA13_SITZE_5']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_results(azdias_transformed, pca, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positively Corr:\n",
    "KOMBIALTER                     0.2217\n",
    "FINANZ_VORSORGER               0.1992: Financially: be preparer\n",
    "CJT_TYP_5                      0.1927  Customer-Journey-Typology relating to the preferred information and buying channels of consumers\n",
    "                                       Advertising- and Cross-Channel-Enthusiast\n",
    "ALTERSKATEGORIE_GROB           0.1909 Age classification: higher -> older\n",
    "CJT_TYP_4                      0.1834 advertisinginterested Online-shopper\n",
    "CJT_TYP_3                      0.1782  advertisinginterested Store-shopper\n",
    "CJT_TYP_6 Advertising-Enthusiast with restricted Cross-Channel-Behaviour \n",
    "\n",
    "Neg Corr\n",
    "SEMIO_PFLICHT                 -0.1870 Traditionally minded (higher == less affinity)\n",
    "FINANZ_UNAUFFAELLIGER         -0.1888 Financial Typology: unremarkable (higher = low)\n",
    "FINANZ_ANLEGER                -0.1915 Investor? (high value = low likelihood)\n",
    "CJT_TYP_2                     -0.2043 Advertising- and Consumptiontraditionalist\n",
    "FINANZ_SPARER                 -0.2101 money saver (low value = very high)\n",
    "CJT_TYP_1                     -0.2119 Advertising- and Consumptionminimalist\n",
    "GENERATION (higher values == younger)\n",
    "\n",
    "So this component is indicating the Customer Journey, Financial Attitude and Age. Higher component indicates higher probability of people being of higher age and money savers. The CJT_TYP_X variables indicate max affinity with the typology when value=1, lowest affinity when value=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_component_pos = ['KOMBIALTER','FINANZ_VORSORGER','CJT_TYP_5','ALTERSKATEGORIE_GROB','CJT_TYP_4','CJT_TYP_3','CJT_TYP_6']\n",
    "third_component_neg = ['SEMIO_PFLICHT','FINANZ_ANLEGER','FINANZ_UNAUFFAELLIGER','CJT_TYP_2','FINANZ_SPARER','CJT_TYP_1','GENERATION']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Clustering\n",
    "\n",
    "### Step 3.1: Apply Clustering to General Population\n",
    "\n",
    "In this step, you will apply k-means clustering to the dataset and use the average within-cluster distances from each point to their assigned cluster's centroid to decide on a number of clusters to keep.\n",
    "\n",
    "- We use sklearn's [KMeans](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans) class to perform k-means clustering on the PCA-transformed data.\n",
    "- Then, we compute the average difference from each point to its assigned cluster's center.\n",
    "- We perform the above two steps for a number of different cluster counts.\n",
    "- We then select a final number of clusters to use, re-fit a KMeans instance to perform the clustering operation. We obtain the cluster assignments for the general demographics data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using these functions instead of model.score to calculate the average distance of the points in the cluster \n",
    "from the centroid. I play with it first in two dimensions and then generalize to n-dimensions\n",
    "\n",
    "Thanks to alphaleonis and Shay.G at https://stackoverflow.com/questions/40828929/sklearn-mean-distance-from-centroid-of-each-cluster for some of the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uster import KMeans \n",
    "\n",
    "# Distance in two dimensions\n",
    "def k_mean_distance(data, cx, cy, i_centroid, cluster_labels):\n",
    "    # Calculate Euclidean distance for each data point assigned to centroid \n",
    "    distances = [np.sqrt((x-cx)**2+(y-cy)**2) for (x, y) in data[cluster_labels == i_centroid]]\n",
    "    # return the mean value\n",
    "    return np.mean(distances)\n",
    "\n",
    "\n",
    "# Distance in n-dimensions\n",
    "def k_mean_distance_n(data, centroid, i_centroid, cluster_labels):\n",
    "    # Calculate Euclidean distance for each data point assigned to centroid \n",
    "    distances = [np.linalg.norm(x - centroid) for x in data[cluster_labels == i_centroid]]\n",
    "    # return the mean value\n",
    "    return np.mean(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "\n",
    "# Distance in two dimensions\n",
    "def k_mean_distance(data, cx, cy, i_centroid, cluster_labels):\n",
    "    # Calculate Euclidean distance for each data point assigned to centroid \n",
    "    distances = [np.sqrt((x-cx)**2+(y-cy)**2) for (x, y) in data[cluster_labels == i_centroid]]\n",
    "    # return the mean value\n",
    "    return np.mean(distances)\n",
    "\n",
    "\n",
    "# Distance in n-dimensions\n",
    "def k_mean_distance_n(data, centroid, i_centroid, cluster_labels):\n",
    "    # Calculate Euclidean distance for each data point assigned to centroid \n",
    "    distances = [np.linalg.norm(x - centroid) for x in data[cluster_labels == i_centroid]]\n",
    "    # return the mean value\n",
    "    return np.mean(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set the max num of clusters to 20\n",
    "max_clusters = 30\n",
    "# This will take a while\n",
    "    \n",
    "# compute the average within-cluster distances.\n",
    "def k_means_score(data, n_clusters):\n",
    "    # Run k-means\n",
    "    kmeans = MiniBatchKMeans(n_clusters=n_clusters, batch_size=20000)\n",
    "    #kmeans = KMeans(n_clusters=n_clusters)\n",
    "    model = kmeans.fit(data)\n",
    "    labels = model.predict(data)\n",
    "    centroids = model.cluster_centers_\n",
    "    \n",
    "    total_distance = []\n",
    "    for i, c in enumerate(centroids):\n",
    "        # Function from above\n",
    "        mean_distance = k_mean_distance_n(data, c, i, labels)\n",
    "        total_distance.append(mean_distance)\n",
    "    return(np.mean(total_distance))\n",
    "  \n",
    "scores = []\n",
    "clusters = list(range(1,max_clusters+1))\n",
    "\n",
    "import progressbar\n",
    "cnter = 0\n",
    "bar = progressbar.ProgressBar(maxval=len(clusters)+1, widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "bar.start()\n",
    "\n",
    "for n_clusters in clusters:\n",
    "    scores.append(k_means_score(azdias_pca, n_clusters))\n",
    "    cnter+=1 \n",
    "    bar.update(cnter)\n",
    "\n",
    "\n",
    "bar.finish()    \n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the elbow method to find the ideal number of clusters\n",
    "plt.plot(clusters, scores, linestyle='--', marker='o', color='b');\n",
    "plt.xlabel('No of Clusters');\n",
    "plt.ylabel('Average Distance');\n",
    "plt.title('Average Distance vs. Clusters');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running MiniBatchKMeans instead of KMeans makes the graph a bit bumpier as the average distance does not always decreases with increasing n. of clusters. However, the general trend is the same. \n",
    "The graph above shows an elbow at 16 clusters and after 25 the average distance  keeps on decreasing at much lower pace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-fit the k-means model with the selected number of clusters and obtain\n",
    "# cluster predictions for the general population demographics data.\n",
    "def apply_KMeans(df_pca, n_clusters):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    kmeans = MiniBatchKMeans(n_clusters=n_clusters, batch_size=50000)\n",
    "    kmeans_model = kmeans.fit(df_pca)\n",
    "    pop_labels = kmeans_model.predict(df_pca)\n",
    "    pop_centroids = kmeans_model.cluster_centers_\n",
    "    \n",
    "    return pop_labels, pop_centroids, kmeans_model\n",
    "\n",
    "gen_labels, gen_centroids, k_model = apply_KMeans(azdias_pca, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file in the current working directory\n",
    "pkl_filename = \"kmeans.pkl\"  \n",
    "with open(pkl_filename, 'wb') as file:  \n",
    "    pickle.dump(k_model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply All Steps to the Customer Data\n",
    "\n",
    "Now that we have clusters and cluster centers for the general population, we can see how the customer data maps on to those clusters. We will use the fits from the general population to clean, transform, and cluster the customer data. Finally we  will interpret how the general population fits apply to the customer data.\n",
    "\n",
    "- Apply the same feature wrangling, selection, and engineering steps to the customer demographics using the `clean_data()` function you created earlier. (You can assume that the customer demographics data has similar meaning behind missing data patterns as the general demographics data.)\n",
    "- Use the sklearn objects from the general demographics data, and apply their transformations to the customers data. That is, you should not be using a `.fit()` or `.fit_transform()` method to re-fit the old objects, nor should you be creating new sklearn objects! Carry the data through the feature scaling, PCA, and clustering steps, obtaining cluster assignments for all of the data in the customer demographics data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = pd.read_csv('arvato_data/Udacity_CUSTOMERS_052018.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Data\n",
    "#'CUSTOMER_GROUP', 'ONLINE_PURCHASE', and 'PRODUCT_GROUP columns will be dropped\n",
    "customers = customers.drop(labels=['CUSTOMER_GROUP', 'ONLINE_PURCHASE', 'PRODUCT_GROUP'], axis=1)\n",
    "customers = clean_data(customers, azdias_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Transformation\n",
    "#customers, imputers, scaler = feature_transformation(customers, azdias_info, imputers, scaler)\n",
    "customers = c_transformer.transform(customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Pca\n",
    "cust_pca = pca.transform(customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering (16 clusters)\n",
    "customer_labels = k_model.predict(cust_pca)\n",
    "customer_centroids = k_model.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.3: Compare Customer Data to Demographics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(gen_labels)/len(gen_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the proportion of data in each cluster for the customer data to the\n",
    "# proportion of data in each cluster for the general population.\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "n_points = len(customer_labels)\n",
    "max_count = np.max(np.bincount(customer_labels))\n",
    "max_prop = max_count / n_points\n",
    "\n",
    "# generate tick mark locations and names\n",
    "tick_props = np.arange(0, max_prop, 0.05)\n",
    "tick_names = ['{:0.2f}'.format(v) for v in tick_props]\n",
    "\n",
    "# Customers the plot\n",
    "base_color = sns.color_palette()[0]\n",
    "sns.countplot(x = customer_labels, color = base_color)\n",
    "plt.yticks(tick_props * n_points, tick_names)\n",
    "plt.ylabel('proportion')\n",
    "plt.xlabel('Cluster')\n",
    "plt.title('Customers Clusters')\n",
    "# Population plot\n",
    "plt.subplot(1, 2, 2)\n",
    "n_points = len(gen_labels)\n",
    "max_count = np.max(np.bincount(gen_labels))\n",
    "max_prop = max_count / n_points\n",
    "\n",
    "sns.countplot(x = gen_labels, color = base_color)\n",
    "plt.yticks(tick_props * n_points, tick_names)\n",
    "plt.ylabel('proportion')\n",
    "plt.xlabel('Cluster')\n",
    "plt.title('General Population Clusters')\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better this ones\n",
    "n_clusters=16\n",
    "plt.figure(figsize=(10,5))\n",
    "# These are the \"Tableau 20\" colors as RGB.    \n",
    "tableau20 = [(31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),    \n",
    "             (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),    \n",
    "             (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),    \n",
    "             (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),    \n",
    "             (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)]    \n",
    "  \n",
    "# Scale the RGB values to the [0, 1] range, which is the format matplotlib accepts.    \n",
    "for i in range(len(tableau20)):    \n",
    "    r, g, b = tableau20[i]    \n",
    "    tableau20[i] = (r / 255., g / 255., b / 255.) \n",
    "\n",
    "population_bins = 100 * np.bincount(gen_labels)/len(gen_labels)\n",
    "customer_bins = 100 * np.bincount(customer_labels)/len(customer_labels)\n",
    "clusters = [x for x in range(0,n_clusters)]\n",
    "ax = plt.subplot(111)\n",
    "w=0.4\n",
    "ax.bar(x=np.array(clusters)-0.2, height=customer_bins, width=w, color=tableau20[0])\n",
    "ax.bar(x=np.array(clusters)+0.2, height=population_bins, width=w, color=tableau20[2])\n",
    "plt.ylabel('Proportion')\n",
    "plt.xlabel('Cluster')\n",
    "plt.xticks(clusters)\n",
    "plt.legend(['Customers', 'General Population'])\n",
    "plt.title('Customers vs Population Clusters')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that clusters 3, 7 and 14 are more represented in the customer dataset compared to the general population, while clusters 1, 2, 6, 10, 13, and 15 are under-represented.\n",
    "\n",
    "From the previous analysis of the PCA we have:\n",
    "- First Component: This component is positively correlated to indicators of wealth like WEALTH (lower values = wealthier), CAMEO_DEUG_2015 (lower values = upper class), ANZ_STATISTISCHE_HAUSHALTE and PLZ8_BAUMAX (# of households in the building, so higher values indicate more density and lower class). LP_STATUS_FEIN indicates higher income individual/families and MOBI_REGIO mobility (here we interpret is as higher mobility indicates less stability therefore lower income class) where higher values indicate lower mobility\n",
    "\n",
    "- Second Component: This component is positively correlated with uppr-middle class indicators like owning a BMW, Merc or Sportcar, mature drivers. We will look at the variables: KBA13_HERST_BMW_BENZ, KBA13_SEG_OBEREMITTELKLASSE, KBA13_MERCEDES, KBA13_BMW, KBA13_SEG_SPORTWAGEN (sport ans luxury cars). And the negatively correlated KBA13_SEG_KLEINWAGEN, KBA13_KMH_140_210 (cheap cars) and KBA13_HALTER_25, KBA13_HALTER_20 (Young drivers)\n",
    "\n",
    "- Third Component: This component is indicating the Customer Journey, positively correlated with younger and more \n",
    "multi-channel enthusiast people and negatively correlated with traditionalist, older people. We will look at the variables KOMBIALTER, FINANZ_VORSORGER, CJT_TYP_5, ALTERSKATEGORIE_GROB, CJT_TYP_4, CJT_TYP_3, CJT_TYP_6. \n",
    "Neg Correlated ones: SEMIO_PFLICHT, FINANZ_UNAUFFAELLIGER, FINANZ_ANLEGER, CJT_TYP_2, FINANZ_SPARER, CJT_TYP_1 and \n",
    "GENERATION (higher values == younger)\n",
    "\n",
    "We will use the .inverse_transform() method of the PCA and StandardScaler objects to transform centroids back to the original data space and interpret the retrieved values directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can have a look at the values of the centroids\n",
    "customer_centroids[:, :3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To be reviewed:\n",
    "So to start in Cluster 3 (4th row) we have a negative value for the first latent feature, suggesting a negative correlation as expected, since positive values in this feature point more towards low-income. The third feature at 3.93 point more towards younger, multi-channel and innvovative, and the 2nd feature moderatively towards higher income.\n",
    "As for Cluster 7 (8th row), similar values for the 1st feature, strong positive for the second one (expensive cars) and moderatively positive for the third component.\n",
    "Similarly in cluster 14 we have -5 in the 1st feature, but this time -2 in the second one (suggesting more younger population this time) and moderate pisitive for the customer journey (third component).\n",
    "If we look at the clusters that are under-represented in the customer dataset, like cluster 2 with a strong first component indicating smaller, not stable households. Similarly cluster 6 has a high positive in the first component and high negative in teh second one (low-income + driving small cars). Cluster 10, 13 and 15 have all negative values in the third component, pointing towards older, traditionalist population in their customer-journey.\n",
    "So from this first look it seems Customers are preferably richer, driving bigger cars but not old too established, prefering people with a more modern approach in their customer journey (multi-channel). \n",
    "\n",
    "Now we can try to tranform centroids back to their original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What kinds of people are part of a cluster that is overrepresented in the\n",
    "# customer data compared to the general population?\n",
    "#features_to_scale\n",
    "centroids_df = pd.DataFrame(pca.inverse_transform(customer_centroids), columns=azdias_columns)\n",
    "centroids_df[features_to_scale] = scaler.inverse_transform(centroids_df[features_to_scale])\n",
    "#over_represented = [3, 7, 14]\n",
    "#under_represented = [1, 2, 6, 10, 13, 15]\n",
    "over_represented = [2, 6, 7, 10, 12]\n",
    "under_represented = [3, 5, 11, 13, 14, 15]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Component Customer Analysis\n",
    "centroids_df[first_component_pos].iloc[over_represented].mean(), centroids_df[first_component_neg].iloc[over_represented].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_df[first_component_pos].iloc[under_represented].mean(), centroids_df[first_component_neg].iloc[under_represented].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at WEALTH (lower values = wealthier) in the clusters that are more represented in the customers dataset the values are all around 2 indicating upper middleclass.\n",
    "Instead in underrepresented clusters this value is sometimes higher than 4, pointing to lower classes.\n",
    "Similarily CAMEO_DEUG_2015 (lower values = upper class), ANZ_STATISTISCHE_HAUSHALTE and PLZ8_BAUMAX (# of households in the building, so higher values indicate more density and lower class)\n",
    "LP_STATUS_FEIN indicates higher income individual/families and MOBI_REGIO mobility (here we interpret is as higher mobility indicates less stability therefore lower income class) where higher values indicate lower mobility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second component\n",
    "centroids_df[second_component_pos].iloc[over_represented].mean(), centroids_df[second_component_neg].iloc[over_represented].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_df[second_component_pos].iloc[under_represented].mean(), centroids_df[second_component_neg].iloc[under_represented].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that in the over represented clusters we have higher shares of luxury vehicles while in the under represented clusters we have higher shares of small, cheaper vehicles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third component\n",
    "centroids_df[third_component_pos].iloc[over_represented].mean(), centroids_df[third_component_neg].iloc[over_represented].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_df[third_component_pos].iloc[under_represented].mean(), centroids_df[third_component_neg].iloc[under_represented].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see at the customer journey related variables CJT_TYP values of 4,5,6 indicate a more on-line shopper and cross channel enthusiast as opposed values of 1 and 2 indicate a more traditionalist approach. So as expected the more innovative type is in the under represented clusters while the more traditionalist in the over represented clusters.\n",
    "Ther we have variables FINANZ_ indicating financial typology where SPARER, UNAUFFAELLIGER are more conservative while ANLEGER is more an investor. VORSORGER indicates financially prepared. GENERATION value is higher in the under represented indicating younger population.\n",
    "More traditionalist, saving conscious and financially prepared are in the over represented cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Supervised Learning Model\n",
    "\n",
    "Now that you've found which parts of the population are more likely to be customers of the mail-order company, it's time to build a prediction model. Each of the rows in the \"MAILOUT\" data files represents an individual that was targeted for a mailout campaign. Ideally, we should be able to use the demographic information from each individual to decide whether or not it will be worth it to include that person in the campaign.\n",
    "\n",
    "The \"MAILOUT\" data has been split into two approximately equal parts, each with almost 43 000 data rows. In this part, you can verify your model with the \"TRAIN\" partition, which i\n",
    "ncludes a column, \"RESPONSE\", that states whether or not a person became a customer of the company following the campaign. In the next part, you'll need to create predictions on the \"TEST\" partition, where the \"RESPONSE\" column has been withheld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maurizio/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (18,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#mailout_train = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TRAIN.csv', sep=';')\n",
    "mailout_train = pd.read_csv('arvato_data/Udacity_MAILOUT_052018_TRAIN.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailout_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = mailout_train.RESPONSE\n",
    "X = mailout_train.drop('RESPONSE', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = clean_data(X, azdias_info, cross_validation=True)\n",
    "x_columns=X.columns\n",
    "#X, imputers, scaler = feature_transformation(X, azdias_info)\n",
    "X = c_transformer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maurizio/anaconda3/lib/python3.6/site-packages/pandas/io/feather_format.py:112: FutureWarning: `nthreads` argument is deprecated, pass `use_threads` instead\n",
      "  return feather.read_dataframe(path, nthreads=nthreads)\n"
     ]
    }
   ],
   "source": [
    "# Save/Load in case the kernel dies\n",
    "#pd.DataFrame(X, columns=x_columns).to_feather('X_mailout_train.feather')\n",
    "X = pd.read_feather('X_mailout_train.feather', nthreads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "from sklearn.metrics import fbeta_score, accuracy_score, roc_auc_score\n",
    "\n",
    "def train_predict(learner, sample_size, X_train, y_train, X_test, y_test): \n",
    "    '''\n",
    "    inputs:\n",
    "       - learner: the learning algorithm to be trained and predicted on\n",
    "       - sample_size: the size of samples (number) to be drawn from training set\n",
    "       - X_train: features training set\n",
    "       - y_train: income training set\n",
    "       - X_test: features testing set\n",
    "       - y_test: income testing set\n",
    "    '''\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # TODO: Fit the learner to the training data using slicing with 'sample_size' using .fit(training_features[:], training_labels[:])\n",
    "    start = time() # Get start time\n",
    "    learner.fit(X_train[:sample_size], y_train[:sample_size])\n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # TODO: Calculate the training time\n",
    "    results['train_time'] = end - start\n",
    "        \n",
    "    # TODO: Get the predictions on the test set(X_test),\n",
    "    #       then get predictions on the first 300 training samples(X_train) using .predict()\n",
    "    start = time() # Get start time\n",
    "    predictions_test = learner.predict(X_test)\n",
    "    predictions_train = learner.predict(X_train)\n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # TODO: Calculate the total prediction time\n",
    "    results['pred_time'] = end - start\n",
    "            \n",
    "    # TODO: Compute accuracy on the first 300 training samples which is y_train[:300]\n",
    "    #results['acc_train'] = accuracy_score(y_train, predictions_train)\n",
    "        \n",
    "    # TODO: Compute accuracy on test set using accuracy_score()\n",
    "    #results['acc_test'] = accuracy_score(y_test, predictions_test)\n",
    "    \n",
    "    # TODO: Compute F-score on the the first 300 training samples using fbeta_score()\n",
    "    #results['f_train'] = fbeta_score(y_train, predictions_train, 0.5)\n",
    "        \n",
    "    # TODO: Compute F-score on the test set which is y_test\n",
    "    #results['f_test'] = fbeta_score(y_test, predictions_test, 0.5)\n",
    "    \n",
    "    results['auc_test'] = roc_auc_score(y_test, predictions_test)\n",
    "    results['auc_train'] = roc_auc_score(y_train, predictions_train)\n",
    "        \n",
    "    # Success\n",
    "    print(\"{} trained on {} samples.\".format(learner.__class__.__name__, sample_size))\n",
    "        \n",
    "    # Return the results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the 'features' and 'income' data into training and testing sets\n",
    "X_train, X_CV, y_train, y_cv = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Show the results of the split\n",
    "print(\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "print(\"Testing set has {} samples.\".format(X_CV.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import the three supervised learning models from sklearn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# TODO: Initialize the three models\n",
    "clf_A = LogisticRegression()\n",
    "\n",
    "#clf_B = AdaBoostClassifier(random_state=0)\n",
    "#clf_B = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(criterion='gini', splitter='best', max_features=0.3,\n",
    "#                                                                 max_depth=2, min_samples_split=0.7, min_samples_leaf=0.004,\n",
    "#                                                                 min_weight_fraction_leaf=0., max_leaf_nodes=None, min_impurity_decrease=0., class_weight=None),\n",
    "#                         random_state=0, learning_rate=0.45, n_estimators=4250)\n",
    "\n",
    "clf_B = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "clf_C = RandomForestClassifier(random_state=42)\n",
    "\n",
    "clf_D = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "\n",
    "# TODO: Calculate the number of samples for 1%, 10%, and 100% of the training data\n",
    "# HINT: samples_100 is the entire training set i.e. len(y_train)\n",
    "# HINT: samples_10 is 10% of samples_100 (ensure to set the count of the values to be `int` and not `float`)\n",
    "# HINT: samples_1 is 1% of samples_100 (ensure to set the count of the values to be `int` and not `float`)\n",
    "samples_100 = len(y_train)\n",
    "samples_10 = len(y_train) // 10\n",
    "samples_1 = len(y_train) // 100\n",
    "\n",
    "# Collect results on the learners\n",
    "results = {}\n",
    "for clf in [clf_A, clf_B, clf_C, clf_D]:\n",
    "    clf_name = clf.__class__.__name__\n",
    "    results[clf_name] = {}\n",
    "    samples = samples_100\n",
    "    #for i, samples in enumerate([samples_1, samples_10, samples_100]):\n",
    "    #results[clf_name] = train_predict(clf, samples, X_train, y_train, X_CV, y_cv)\n",
    "    results[clf_name] = cross_val_score(clf, X, y, cv=5, scoring='roc_auc')\n",
    "\n",
    "# Run metrics visualization for the three supervised learning models chosen\n",
    "#vs.evaluate(results, accuracy, fscore)\n",
    "for clf in [clf_A, clf_B, clf_C, clf_D]:\n",
    "    clf_name = clf.__class__.__name__\n",
    "    #print(clf_name + \" trained in {} seconds\".format(results[clf_name]['train_time']))\n",
    "    #print(clf_name + \" Fbeta Training {}\".format(results[clf_name]['f_train']))\n",
    "    #print(clf_name + \" Fbeta Test {}\\n\".format(results[clf_name]['f_test']))\n",
    "    #print(clf_name + \" ROC AUC Training {}\\n\".format(results[clf_name]['auc_train']))\n",
    "    print(clf_name + \" ROC AUC Test {}\\n\".format(results[clf_name]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "#from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "clf = XGBClassifier(random_state=42)\n",
    "scores = cross_val_score(clf, X, y, cv=5, scoring='roc_auc')\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will compare few methods by reusing the code from the \"Finding Donors\" project\n",
    "# Learning Curves, to have a look at Bias/Variance\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from plot_learning_curve import plot_learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "\n",
    "#f_point5_scorer = make_scorer(fbeta_score, beta=0.5)\n",
    "\n",
    "roc_scorer = make_scorer(roc_auc_score)\n",
    "\n",
    "roc_scorer = 'roc_auc'\n",
    "# TODO: Initialize the three models\n",
    "clf_A = DecisionTreeClassifier(random_state=42)\n",
    "clf_B = RandomForestClassifier(random_state=42)\n",
    "clf_C = AdaBoostClassifier(random_state=42)\n",
    "clf_D = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "#X = pd.concat([X_train, X_CV])\n",
    "#y = pd.concat([y_train,  y_cv])\n",
    "plt.figure(figsize = [20, 5])\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "plt.subplot(1, 4, 1)\n",
    "plot_learning_curve(clf_A, 'Decision Tree Learning Curve', X, y, scorer=roc_scorer, cv=cv, ylim=(0.5, 1), n_jobs=2)\n",
    "\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "plt.subplot(1, 4, 2)\n",
    "plot_learning_curve(clf_B, 'Random Forest Learning Curve', X, y, scorer=roc_scorer, cv=cv, ylim=(0.5, 1), n_jobs=2)\n",
    "\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "plt.subplot(1, 4, 3)\n",
    "plot_learning_curve(clf_C, 'Ada Boost Learning Curve', X, y, scorer=roc_scorer, cv=cv, ylim=(0.5, 1), n_jobs=2)\n",
    "\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "plt.subplot(1, 4, 4)\n",
    "plot_learning_curve(clf_D, 'Gradient Boosting Learning Curve', X, y, scorer=roc_scorer, cv=cv, ylim=(0.5, 1), n_jobs=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.concatenate((X_train, X_CV))\n",
    "xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Kaggle Competition\n",
    "\n",
    "Now that you've created a model to predict which individuals are most likely to respond to a mailout campaign, it's time to test that model in competition through Kaggle. If you click on the link [here](http://www.kaggle.com/t/21e6d45d4c574c7fa2d868f0e8c83140), you'll be taken to the competition page where, if you have a Kaggle account, you can enter. If you're one of the top performers, you may have the chance to be contacted by a hiring manager from Arvato or Bertelsmann for an interview!\n",
    "\n",
    "Your entry to the competition should be a CSV file with two columns. The first column should be a copy of \"LNR\", which acts as an ID number for each individual in the \"TEST\" partition. The second column, \"RESPONSE\", should be some measure of how likely each individual became a customer  this might not be a straightforward probability. As you should have found in Part 2, there is a large output class imbalance, where most individuals did not respond to the mailout. Thus, predicting individual classes and using accuracy does not seem to be an appropriate performance evaluation method. Instead, the competition will be using AUC to evaluate performance. The exact values of the \"RESPONSE\" column do not matter as much: only that the higher values try to capture as many of the actual customers as possible, early in the ROC curve sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailout_test = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TEST.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
